{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim是一种通过检查词汇模式（或更高级别的结构，如语句或文档）来发现文档语义结构（Semantic Structure）的工具。 gensim通过语料库 --- 一组文本文档，并在语料库中生成文本的向量表示（Vector Representation of the Text）来实现这一点。 然后，文本的向量表示可用于训练模型 --- 它是用于创建不同的文本数据（蕴含语义）表示的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关概念：\n",
    "\n",
    "1. 语料（Corpus）：一组原始文本的集合，用于无监督地训练文本主题的隐层结构。在Gensim中，Corpus通常是一个可迭代的对象（比如列表）。每    一次迭代返回一个可用于表达文本对象的稀疏向量。\n",
    "\n",
    "2. 向量（Vector）：由一组文本特征构成的列表。是一段文本在Gensim中的内部表达。在向量空间模型中，每个文档被表示成了一组特征，比如，一个    单一的特征可能被视为一个问答对。\n",
    "\n",
    "3. 稀疏向量（SparseVector）：通常，大部分问题的答案都是0，为了节约空间，我们会从文档表示中省略他们，向量中的每一个元素是一个(key,      value)的元组，比如（1,3），（2,4），（5,0），其中（5,0）是一个稀疏向量，在表示是会被忽略。\n",
    "\n",
    "4. 模型（Model）：是一个抽象的术语。定义了两个向量空间的变换（即从文本的一种向量表达变换为另一种向量表达）。\n",
    "\n",
    "\n",
    "把几个概念组织起来表述：gensim可以通过读取一段语料，输出一个向量，表示文档中的一个词。为了节约空间，通常稀疏的词向量会被忽略，剩下的词向量则可以用来训练各种模型，即从原有的文本表达转向另一种文本表达。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaohaonan/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jieba\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义 function，利用 Jieba 给句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数，返回分词列表(jieba)\n",
    "# string -> string\n",
    "def trans(x):\n",
    "    x = list(jieba.cut(x))\n",
    "    return \" \".join(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jieba Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/x1/gwtt25l1289_g68tqjk3626c0000gn/T/jieba.cache\n",
      "Loading model cost 0.832 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王八蛋 老板 黄鹤 和 他 的 小姨子 跑 了\n"
     ]
    }
   ],
   "source": [
    "s = '王八蛋老板黄鹤和他的小姨子跑了'\n",
    "print(trans(s))\n",
    "\n",
    "# \"他的\" 是分开的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王八蛋 老板 黄鹤 和 他的 小姨子 跑 了\n"
     ]
    }
   ],
   "source": [
    "# \"他的\"合并\n",
    "jieba.suggest_freq('他的', True)\n",
    "print(trans(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: 语料（Corpus）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个语料库是数字文档的集合（A Collection of Digital Documents）。 \n",
    "这个集合是gensim的输入，它将从中推断文档的结构或主题。\n",
    "从语料库中推断出的潜在结构（Latent Structure）可用于将主题分配给先前不存在于仅用于训练的语料库中的新文档。 \n",
    "出于这个原因，我们也将此集合称为训练语料库（Training Corpus）。 这个过程不需要人工干预（比如手动给文档打标签） \n",
    "--- 因为主题分类是无监督的（Unsupervised）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = list()   # content for the whole data\n",
    "\n",
    "\n",
    "f = open(\"input.txt\", \"r\", encoding=\"utf8\")\n",
    "# f = open(\"/input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "# Add content to list (base on “A”, “B”, “C”)\n",
    "for line in f:\n",
    "    x = json.loads(line)    \n",
    "    raw_corpus.append(x[\"A\"])   \n",
    "    raw_corpus.append(x[\"B\"])\n",
    "    raw_corpus.append(x[\"C\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用 Jieba 对语料库中的文档进行分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a List\n",
    "for a in range(0, len(raw_corpus)):\n",
    "    raw_corpus[a] = trans(raw_corpus[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除常用词以及分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a stopwrods list\n",
    "stoplist = [i.strip() for i in open('stopword.txt',encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words according to the stoplist\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in raw_corpus]\n",
    "\n",
    "# Non-stoplist version\n",
    "# texts = [[word for word in document.lower().split()]\n",
    "#          for document in raw_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算词频\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅保留词频数高于1的词汇\n",
    "'''\n",
    "processed_corpus = [[line 1 A], [line 1 B], [line 1 C],\n",
    "                    [line 2 A], [line 2 B], [line 2 C]\n",
    "                    ...\n",
    "                    [line 500 A], [line 500 B], [line 500 C]] \n",
    "\n",
    "'''\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n",
      "Dictionary(4347 unique tokens: ['1', '12015', '2', '2015', '2016']...)\n"
     ]
    }
   ],
   "source": [
    "# 将语料库中的每个词汇与唯一的整数ID相关联\n",
    "# Create a dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: 向量空间（Vector Space）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了推断语料库中的潜在结构（Latent Structure），我们需要一种可用于数学操作（比如，加减乘除等运算）的文档表示方法。一种方法是将每个文档表示为向量。有各种用于创建文档的向量表示的方法，其中一个简单的方法是词袋模型(Bag-of-Words Model)。\n",
    "在词袋模型下，每个文档由包含字典中每个单词的频率计数的向量表示。例如，给定一个包含词汇['咖啡'，'牛奶'，'糖果'，'勺子']的字典，那么，一个由字符串'咖啡 牛奶 糖果 勺子'组成的文档可以用向量表示为[2 ，1，0,0]，其中向量的元素（按顺序）对应文档中出现的“咖啡”，“牛奶”，“糖”和“勺子”。向量的长度是字典中的词汇数。词袋模型的一个主要特性是它完全忽略了编码文档（the Encoded Document ）中的词汇顺序，这就是词袋模型的由来。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# key -> number; value -> words\n",
    "# See the structure of the dictionary\n",
    "\n",
    "# print(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Example (将文本用向量表示)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们想要对“通远乡霞山区国家石头专用部门王八蛋周总周总”这个语句进行向量化（请注意，该语句不在我们原来的语料库中）。 我们可以使用dictionary的doc2bow方法为该语句创建词袋表示，该方法返回词汇计数的稀疏表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4301, 2), (4308, 1), (4317, 1), (4325, 1), (4327, 1), (4342, 1), (4346, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"通远乡 霞山区 国家 石头 专用 部门 王八蛋 周总 周总\"  #已分词，便于后续处理\n",
    "\n",
    "# 利用创建好的 dictionary 来匹配每个 word, (id, frequency)  \n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split()) \n",
    "\n",
    "new_vec\n",
    "\n",
    "# 每个元组中的第一个元素对应字典中的词汇ID，第二个条目对应于该词汇的计数。此向量仅包含实际出现在文档中的词汇。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用字典将整个原始语料库 (processed_corpus) 转换为向量列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file 每一篇文章都转换成向量\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# bow_corpus\n",
    "# print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: 模型 （ TF-IDF , LSI ，LDA ，RP）\n",
    "\n",
    "## 利用训练好的 Model 将原始文本转好成相应 Model 的向量空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经对测试语料库进行了向量化，我们可以开始使用models对其进行转换了。 我们使用模型作为抽象术语，指的是从一个文档表示到另一个文档表示的转换。 在gensim中，文档表示为向量，因而模型可以被认为是两个向量空间之间的转换。 从训练语料库中学习这种转换的细节。\n",
    "\n",
    "一个简单的模型示例是TF-IDF。 TF-IDF模型将向量从词袋表示（Bag-of-Words Representation）转换为向量空间，其中频率计数根据语料库中每个单词的相对稀有度（the relative rarity of each word in the corpus）进行加权。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=1500, num_nnz=194228)\n",
      "利用 TF-IDF 转换好的样本size == 原样本size 1500\n",
      "\n",
      "\n",
      "LsiModel(num_terms=4347, num_topics=12, decay=1.0, chunksize=20000)\n",
      "利用 LSI 转换好的样本size == 原样本size 1500\n",
      "\n",
      "\n",
      "LdaModel(num_terms=4347, num_topics=13, decay=0.5, chunksize=2000)\n",
      "利用 LDA 转换好的样本size == 原样本size 1500\n",
      "[(0, 0.012681211), (1, 0.012681254), (2, 0.012681115), (3, 0.012681498), (4, 0.012681212), (5, 0.012681256), (6, 0.012681219), (7, 0.84782505), (8, 0.012681265), (9, 0.0126811825), (10, 0.012681193), (11, 0.012681313), (12, 0.012681206)]\n",
      "\n",
      "\n",
      "RpModel(num_terms=4347, num_topics=16)\n",
      "利用 RP 转换好的样本size == 原样本size 1500\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn\n",
    "# doc2bow()与在CountVectorizer上调用transform()有类似的作用\n",
    "# doc2bow()也可以像fit_transform()那样运作\n",
    "\n",
    "\n",
    "# 训练模型 (TF-IDF, LSI, LDA ,RP)\n",
    "# 利用训练好的 Model 将原始文本转好成相应 Model 的向量空间\n",
    "\n",
    "# ------------ TF-IDF ------------\n",
    "# 初始化 tf-idf 模型，在测试语料库上进行训练\n",
    "# input: bow_corpus (已经利用词典将每个词语转化成向量列表格式)\n",
    "tfidf_model = models.TfidfModel(bow_corpus)\n",
    "print(tfidf_model)      # TfidfModel(num_docs=1500, num_nnz=194228)\n",
    "\n",
    "# --- 利用模型 (TF-IDF)，将整个语料库中每一个sample转为tfidf表示方法 ---\n",
    "corpus_tfidf = tfidf_model[bow_corpus]\n",
    "\n",
    "print(\"利用 TF-IDF 转换好的样本size == 原样本size\", len(corpus_tfidf))\n",
    "# print(corpus_tfidf[0])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# ------------ LSI ------------\n",
    "\n",
    "# 初始化 LSI 模型，在测试语料库上进行训练 （# 184, 13）（利用 corpus_tfidf）\n",
    "lsi_model = models.LsiModel(bow_corpus, id2word=dictionary, num_topics= 12)\n",
    "print(lsi_model)\n",
    "\n",
    "# --- 利用模型 (LSI)，将整个语料库中每一个sample转为LSI表示方法 ---\n",
    "corpus_lsi = lsi_model[corpus_tfidf]\n",
    "print(\"利用 LSI 转换好的样本size == 原样本size\", len(corpus_lsi))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# --- LDA ---\n",
    "# 初始化 LDA 模型，在测试语料库上进行训练 （# 13）\n",
    "lda_model = models.LdaModel(bow_corpus, id2word=dictionary, num_topics= 13)\n",
    "print(lda_model)\n",
    "\n",
    "# --- 利用模型 (LDA)，将整个语料库中每一个sample转为LDA表示方法 ---\n",
    "#### using corpus_tfidf ####\n",
    "corpus_lda = lda_model[corpus_tfidf]\n",
    "\n",
    "print(\"利用 LDA 转换好的样本size == 原样本size\", len(corpus_lda))\n",
    "print(corpus_lda[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# ------------ RP (Random Projections) ------------\n",
    "\n",
    "# 初始化 RP 模型，在测试语料库上进行训练 （利用 corpus_tfidf）\n",
    "rp_model = models.RpModel(corpus_tfidf, id2word=dictionary, num_topics= 16\n",
    "                         )\n",
    "print(rp_model)\n",
    "\n",
    "# --- 利用模型 (RP)，将整个语料库中每一个sample转为RP表示方法 ---\n",
    "corpus_rp = rp_model[corpus_tfidf]\n",
    "print(\"利用 RP 转换好的样本size == 原样本size\", len(corpus_rp))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.064*\"被告\" + 0.040*\"1\" + 0.037*\"原告\" + 0.030*\"月\" + 0.025*\"借款\"\n"
     ]
    }
   ],
   "source": [
    "# LDA 分类结果 (最具有代表性的5个)\n",
    "print(lda_model.print_topic(1, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子: TF-IDF模型将向量从词袋表示 转化成TFIDF向量（的迭代器）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 “通远乡霞山区国家石头专用部门王八蛋周总周总\" 进行转换\n",
    "\n",
    "# ------------ TF-IDF模型返回元组列表 ------------\n",
    "# print(\"利用TF-IDF模型返回元组列表，每个元组的第一个元素是词汇ID，第二个条目是TF-IDF加权值.\") \n",
    "\n",
    "# 每个元组的第一个元素是词汇ID，第二个条目是TF-IDF加权值。 \n",
    "# tfidf_model[dictionary.doc2bow(\"通远乡 霞山区 国家 石头 专用 部门 王八蛋 周总 周总\".split())]\n",
    "\n",
    "\n",
    "\n",
    "# ------------ LSI 模型返回元组列表 ------------\n",
    "# lsi_model[dictionary.doc2bow(\"通远乡 霞山区 国家 石头 专用 部门 王八蛋 周总 周总\".split())]\n",
    "\n",
    "\n",
    "# ------------ LDA 模型返回元组列表 ------------\n",
    "# print(\"利用LDA模型返回元组列表，每个元组的第一个元素是词汇ID，第二个条目是TF-IDF加权值.\") \n",
    "# lda_model[dictionary.doc2bow(\"通远乡 霞山区 国家 石头 专用 部门 王八蛋 周总 周总\".split())]\n",
    "\n",
    "\n",
    "# ------------ RP 模型返回元组列表 ------------\n",
    "# print(\"利用RP模型返回元组列表，每个元组的第一个元素是词汇ID，第二个条目是TF-IDF加权值.\") \n",
    "# rp_model[dictionary.doc2bow(\"通远乡 霞山区 国家 石头 专用 部门 王八蛋 周总 周总\".split())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: 相似度的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ 创建索引 ------------\n",
    "\n",
    "# tfidf method\n",
    "index_tfidf = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "# index_tfidf = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary.keys()))\n",
    "\n",
    "# LSI method\n",
    "index_lsi = similarities.MatrixSimilarity(corpus_lsi)\n",
    "\n",
    "\n",
    "# LDA method\n",
    "index_lda = similarities.MatrixSimilarity(corpus_lda)\n",
    "\n",
    "\n",
    "# RP method\n",
    "index_rp = similarities.MatrixSimilarity(corpus_rp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_tfidf MatrixSimilarity<1500 docs, 4342 features>\n",
      "index_lsi MatrixSimilarity<1500 docs, 12 features>\n",
      "index_lda MatrixSimilarity<1500 docs, 13 features>\n",
      "index_rp MatrixSimilarity<1500 docs, 16 features>\n"
     ]
    }
   ],
   "source": [
    "print(\"index_tfidf\", index_tfidf)\n",
    "print(\"index_lsi\", index_lsi)\n",
    "print(\"index_lda\", index_lda)\n",
    "print(\"index_rp\", index_rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : 利用 TF_IDF , LSI , LDA 计算文本中 line1 A B C 相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ tfidf method ------\n",
      "[array([1.0000001 , 0.00720893, 0.00726172], dtype=float32)]\n",
      "\n",
      "\n",
      "------ lsi method ------\n",
      "[ 1.         -0.44931525 -0.16945885]\n",
      "------ lda method ------\n",
      "0.030291621010760588\n",
      "0.8152550790202141\n",
      "[ 1.         -0.12088701 -0.3288839 ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ tfidf method ------------------------\n",
    "print(\"------ tfidf method ------\")\n",
    "\n",
    "# 第一篇文章的向量\n",
    "A_tfidf = [corpus_tfidf[0]]\n",
    "\n",
    "# line-A 和 整个line1 比较\n",
    "sim_tfidf = [index_tfidf[element][0:3] for element in A_tfidf]\n",
    "print(sim_tfidf)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# ------------------------ lsi method ------------------------\n",
    "print(\"------ lsi method ------\")\n",
    "\n",
    "# 第一篇文章的向量\n",
    "A_lsi = corpus_lsi[0]\n",
    "\n",
    "# line-A 和 整个line1 比较\n",
    "sim_lsi = index_lsi[A_lsi][0:3] \n",
    "print(sim_lsi)\n",
    "\n",
    "\n",
    "# ------------------------ lsi method ------------------------\n",
    "print(\"------ lda method ------\")\n",
    "\n",
    "# 第一篇文章的向量\n",
    "A_lda = corpus_lda[0]\n",
    "B_lda = corpus_lda[1]\n",
    "C_lda = corpus_lda[2]\n",
    "\n",
    "# Cosine similarity\n",
    "sim_lda_cosine = gensim.matutils.cossim(A_lda, B_lda)\n",
    "\n",
    "print(sim_lda_cosine)\n",
    "\n",
    "# Hellinger distance\n",
    "dense1 = gensim.matutils.sparse2full(A_lda, lda_model.num_topics)\n",
    "dense2 = gensim.matutils.sparse2full(B_lda, lda_model.num_topics)\n",
    "sim_hd = np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())\n",
    "print(sim_hd)\n",
    "\n",
    "\n",
    "# ------------------------ rp method ------------------------\n",
    "\n",
    "# 第一篇文章的向量\n",
    "A_rp = corpus_rp[0]\n",
    "\n",
    "# line-A 和 整个line1 比较\n",
    "sim_rp = index_rp[A_rp][0:3] \n",
    "print(sim_rp)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for each comparsion example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### TF-IDF ####################\n",
    "\n",
    "# i: acc (start of the list)\n",
    "# j: acc (end of the list)\n",
    "# size: size of the list\n",
    "\n",
    "def tfidf_compare(i, j, size):\n",
    "    while i < size:  \n",
    "        \n",
    "#----------------- Built in sim function ----------------------\n",
    "        \n",
    "        # 待比较的文档 （A）\n",
    "        A = [corpus_tfidf[i]]\n",
    "    \n",
    "        # 相似度计算\n",
    "        j = i + 3\n",
    "        sims = [index_tfidf[new_vec_tfidf][i:j] for new_vec_tfidf in A]\n",
    "    \n",
    "        v1 = sims[0][1]\n",
    "        v2 = sims[0][2]\n",
    "                \n",
    "        if v1 > v2:\n",
    "            print(\"B\", file=ouf)\n",
    "        else:\n",
    "            print(\"C\", file=ouf)\n",
    "        i = j\n",
    "    ouf.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#################### LSI ####################\n",
    "\n",
    "# i: acc (start of the list)\n",
    "# j: acc (end of the list)\n",
    "# size: size of the list\n",
    "\n",
    "def lsi_compare(i, j, size):\n",
    "    while i < size:\n",
    "        # 待比较的文档 （A）\n",
    "        A = corpus_lsi[i]\n",
    "        j = i + 3\n",
    "        # 相似度计算\n",
    "        sims = index_lsi[A][i:j] \n",
    "        v1 = sims[1]\n",
    "        v2 = sims[2]\n",
    "        \n",
    "        if v1 > v2:\n",
    "            print(\"B\", file=ouf)\n",
    "        else:\n",
    "            print(\"C\", file=ouf)\n",
    "        i = j\n",
    "    ouf.close()\n",
    "\n",
    "    \n",
    "#################### LDA (Hellinger distance) ####################\n",
    "\n",
    "# i: acc (start of the list)\n",
    "# j: acc (end of the list)\n",
    "\n",
    "def lda_compare(i, j):\n",
    "    while i < size:\n",
    "        # 待比较的文档 （A）\n",
    "        A = corpus_lda[i]\n",
    "        B = corpus_lda[i + 1]\n",
    "        C = corpus_lda[i + 2]\n",
    "        j = i + 3\n",
    "        # 相似度计算\n",
    "        \n",
    "        dense_A = gensim.matutils.sparse2full(A, lda_model.num_topics)\n",
    "        dense_B = gensim.matutils.sparse2full(B, lda_model.num_topics)\n",
    "        dense_C = gensim.matutils.sparse2full(C, lda_model.num_topics)\n",
    "\n",
    "        distance_AB = np.sqrt(0.5 * ((np.sqrt(dense_A) - np.sqrt(dense_B))**2).sum())\n",
    "        distance_AC = np.sqrt(0.5 * ((np.sqrt(dense_A) - np.sqrt(dense_C))**2).sum())\n",
    "\n",
    "        \n",
    "        if distance_AB > distance_AC:\n",
    "            print(\"C\", file=ouf)\n",
    "        else:\n",
    "            print(\"B\", file=ouf)\n",
    "        i = j\n",
    "    ouf.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#################### RP ####################\n",
    "\n",
    "# i: acc (start of the list)\n",
    "# j: acc (end of the list)\n",
    "# size: size of the list\n",
    "\n",
    "def rp_compare(i, j, size):\n",
    "    while i < size:\n",
    "        # 待比较的文档 （A）\n",
    "        A = corpus_rp[i]\n",
    "        j = i + 3\n",
    "        # 相似度计算\n",
    "        sims = index_rp[A][i:j] \n",
    "        v1 = sims[1]\n",
    "        v2 = sims[2]\n",
    "        \n",
    "        if v1 > v2:\n",
    "            print(\"B\", file=ouf)\n",
    "        else:\n",
    "            print(\"C\", file=ouf)\n",
    "        i = j\n",
    "    ouf.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouf = open(\"output.txt\", \"w\", encoding=\"utf8\")\n",
    "# ouf = open(\"/output/output.txt\", \"w\", encoding=\"utf8\")\n",
    "size = len(raw_corpus)\n",
    "\n",
    "# ------ TF-IDF ------\n",
    "\n",
    "# tfidf_compare(0, 0, size)\n",
    "\n",
    "# ------ LSI ------\n",
    "# lsi_compare(0, 0, size)\n",
    "\n",
    "\n",
    "# ------ LDA ------\n",
    "# lda_compare(0, 0)\n",
    "\n",
    "\n",
    "# ------ RP ------\n",
    "rp_compare(0, 0, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of C is  248\n",
      "Number of B is  252\n",
      "Model accuarcy is  0.504\n"
     ]
    }
   ],
   "source": [
    "def eval():\n",
    "    ouf = open(\"output.txt\", \"r\", encoding=\"utf8\")    \n",
    "    B = 0\n",
    "    C = 0\n",
    "    x = ouf.readlines()\n",
    "    for i in x:\n",
    "        if i == 'B\\n':\n",
    "            B = B + 1\n",
    "        if i == 'C\\n':\n",
    "            C = C + 1\n",
    "        else: \n",
    "            continue\n",
    "\n",
    "    print(\"Number of C is \", C)\n",
    "    print(\"Number of B is \", B)\n",
    "    print(\"Model accuarcy is \", B/(C+B))\n",
    "ouf.close()   \n",
    "\n",
    "eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
